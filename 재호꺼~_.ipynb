{
  "metadata": {
    "name": "재호꺼~",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datajcthemax/playdata/blob/main/%E1%84%8C%E1%85%A2%E1%84%92%E1%85%A9%E1%84%81%E1%85%A5~_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0X0kT92-lUQ"
      },
      "source": [
        "### 데이터\b사이언스 관련 업종 연봉\n",
        "![LGTM](https://i.lgtm.fun/2h0l.png)\n",
        "\n",
        "- 데이터 위치 s3://pd24/data/salary\n",
        "- 데이터 형식 parquet\n",
        "- SQL 참고 링크 : [https://www.w3schools.com/sql/](https://www.w3schools.com/sql/)\n",
        "- 데이터 출처 링크: [https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023](https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj8sxWNU-lUU"
      },
      "source": [
        "### DataFrame Schema\n",
        "```\n",
        "%spark.pyspark\n",
        "df.printSchema()\n",
        "\n",
        "root\n",
        " |-- work_year: long (nullable = true)\n",
        " |-- experience_level: string (nullable = true)\n",
        " |-- employment_type: string (nullable = true)\n",
        " |-- job_title: string (nullable = true)\n",
        " |-- salary: long (nullable = true)\n",
        " |-- salary_currency: string (nullable = true)\n",
        " |-- salary_in_usd: long (nullable = true)\n",
        " |-- employee_residence: string (nullable = true)\n",
        " |-- remote_ratio: long (nullable = true)\n",
        " |-- company_location: string (nullable = true)\n",
        " |-- company_size: string (nullable = true)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY9yo_il-lUV"
      },
      "source": [
        "### 사용방법\n",
        "- %spark.pyspark 로 첫줄 지정\n",
        "\n",
        "``` python\n",
        "# parquet 읽어오기\n",
        "df = spark.read.parquet(\"s3://pd24/data/salary/*\")\n",
        "\n",
        "# sql 쿼리를 위한 임시 테이블 만들기\n",
        "df.createOrReplaceTempView(\"temp_jai_salary\")\n",
        "```\n",
        "\n",
        "- %spark.sql 로 첫줄 지정\n",
        "\n",
        "``` sql\n",
        "SELECT\n",
        "    work_year\n",
        "FROM temp_jai_salary\n",
        "LIMIT 10;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeH9JX-_-lUV"
      },
      "source": [
        "#csv -> parquet (using google drive and google colab)\n",
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# read the csv file\n",
        "data = pd.read_csv('salary.csv')\n",
        "\n",
        "# convert DataFrame to Apache Arrow Table\n",
        "table = pa.Table.from_pandas(data)\n",
        "\n",
        "# write the Table to Parquet format and save it\n",
        "pq.write_table(table, 'salary.parquet')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ffYq8p-lUW"
      },
      "source": [
        "#parquet file을 s3로 보내기\n",
        "\n",
        "\n",
        "```bash\n",
        "aws s3 cp /Users/jaihocho/Downloads/salary.parquet s3://pd24/data/salary/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "5bUhSDSa-lUW"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df = spark.read.parquet(\"s3://pd24/data/salary/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "n-PUWFsi-lUY"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "df.createOrReplaceTempView(\"temp_jai_salary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "oecyGXYw-lUY"
      },
      "outputs": [],
      "source": [
        "\n",
        "%spark.sql\n",
        "SELECT experience_level, COUNT(*) AS count\n",
        "FROM temp_jai_salary\n",
        "GROUP BY experience_level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "JvK2342r-lUZ"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT experience_level, AVG(salary_in_usd) AS avg_salary\n",
        "FROM temp_jai_salary\n",
        "GROUP BY experience_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "p0ZU-cZg-lUZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "%spark.sql\n",
        "SELECT employee_residence, COUNT(*) AS count\n",
        "FROM temp_jai_salary\n",
        "GROUP BY employee_residence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "zObyIi33-lUZ"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT company_location, AVG(salary_in_usd) AS avg_salary_usd\n",
        "FROM temp_jai_salary\n",
        "GROUP BY company_location\n",
        "ORDER BY avg_salary_usd DESC\n",
        "LIMIT 5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "26oKHCh6-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT job_title, AVG(salary_in_usd) AS avg_salary_usd\n",
        "FROM temp_jai_salary\n",
        "GROUP BY job_title\n",
        "ORDER BY avg_salary_usd DESC\n",
        "LIMIT 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "BSLpR6gc-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT job_title, AVG(salary_in_usd) AS avg_salary_usd\n",
        "FROM temp_jai_salary\n",
        "GROUP BY job_title\n",
        "ORDER BY avg_salary_usd ASC\n",
        "LIMIT 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "Fa4QLgXK-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT SUM(CASE WHEN remote_ratio > 0 THEN 1 ELSE 0 END) AS remote_workers,\n",
        "       SUM(CASE WHEN remote_ratio = 0 THEN 1 ELSE 0 END) AS offline_workers,\n",
        "       COUNT(*) AS total_workers,\n",
        "       SUM(CASE WHEN remote_ratio > 0 THEN 1 ELSE 0 END) / SUM(CASE WHEN remote_ratio = 0 THEN 1 ELSE 0 END) AS ratio_remote_to_offline\n",
        "FROM temp_jai_salary\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "j8efp2cw-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT company_size, COUNT(*) AS count\n",
        "FROM temp_jai_salary\n",
        "GROUP BY company_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "nukG3Qpy-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT work_year, COUNT(*) AS count\n",
        "FROM temp_jai_salary\n",
        "GROUP BY work_year\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "j4E5iHz6-lUa"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "SELECT job_title, COUNT(*) AS count\n",
        "FROM temp_jai_salary\n",
        "GROUP BY job_title\n",
        "ORDER BY count DESC\n",
        "LIMIT 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIFi4KcS-lUa"
      },
      "source": [
        "#결론\n",
        "데이터사이언스 관련 업종 종사자는 미국에 가장 많고 평균연봉이 가장 높은 두나라는 이스라엘과 푸에토리코이다. 데사로 잘 먹고 잘 사려면 이스라엘로 가면됨. 시니어급 경력자들이 압도적으로 많고 신입이 가장 적다. 신입 평균연봉은 7에서 8만불사이다. 평균연봉이 압도적으로 높은직업은 데이터사이언스 테크리드 이고 나머지 상위 10직업들은 연봉 도긴개긴이다. 데사 관련 직종중 가장 평균연봉이 적은 직업은 파워 BI 개발자다.  재택과 출근 비율은 거의 50대 50이다. 회사 사이즈는 중간이 압도적으로 많다. 2021~2022년 사이에 데사 관련 종사자가 거의 8배가 증가 했으나 22~23년은 약간의 증가만 있었다.데사 관련 가장 흔한 직업은 데이터 엔지니어, 데이터  사이언티스트, 데이터 분석가 순이다.\n",
        "\n",
        "![LGTM](https://i.lgtm.fun/2h3c.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHiHIufI-lUb"
      },
      "source": []
    }
  ]
}